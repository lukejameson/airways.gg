# Production environment
# Usage: docker compose -f docker-compose.prod.yml --env-file .env.prod up -d

services:
  app:
    build:
      context: .
      dockerfile: apps/web/Dockerfile
    ports:
      - "3002:3000"
    environment:
      - DATABASE_URL=${DATABASE_URL}
      - REDIS_URL=${REDIS_URL}
      - NODE_ENV=production
      - ADDRESS_HEADER=CF-Connecting-IP
      - BODY_SIZE_LIMIT=10M
      - ML_SERVICE_URL=http://ml-service:8000
      - SESSION_SECRET=${SESSION_SECRET}
      - DOMAIN=${DOMAIN:?DOMAIN is required}
      - ORIGIN=https://${DOMAIN:?DOMAIN is required}
      - PUBLIC_BUY_ME_A_COFFEE_URL=${PUBLIC_BUY_ME_A_COFFEE_URL}
      - PUBLIC_UMAMI_WEBSITE_ID=${PUBLIC_UMAMI_WEBSITE_ID}
      - PUBLIC_UMAMI_URL=${PUBLIC_UMAMI_URL}
    labels:
      - 'traefik.enable=true'
      - 'traefik.http.routers.delaysgg-prod.rule=Host(`${DOMAIN}`)'
      - 'traefik.http.routers.delaysgg-prod.entrypoints=websecure'
      - 'traefik.http.routers.delaysgg-prod.tls=true'
      - 'traefik.http.routers.delaysgg-prod.tls.certresolver=letsencrypt'
      - 'traefik.http.services.delaysgg-prod.loadbalancer.server.port=3000'
    depends_on:
      - ml-service
    networks:
      - web
    restart: unless-stopped

  aurigny-scraper:
    build:
      context: .
      dockerfile: apps/aurigny-scraper/Dockerfile
    environment:
      - DATABASE_URL=${DATABASE_URL}
      - SCRAPER_MAX_RETRIES=${SCRAPER_MAX_RETRIES:-3}
      - NODE_ENV=production
      - PROXY_ENABLED=${PROXY_ENABLED:-true}
      - PROXY_USERNAME=${PROXY_USERNAME:-}
      - PROXY_PASSWORD=${PROXY_PASSWORD:-}
      - PROXY_HOSTS=${PROXY_HOSTS:-}
      # Operating window — hard cutoff hour (Guernsey local, 0-23) and wake offset before first flight
      - SCRAPER_CUTOFF_HOUR=${SCRAPER_CUTOFF_HOUR:-23}
      - SCRAPER_WAKE_OFFSET_MINS=${SCRAPER_WAKE_OFFSET_MINS:-30}
      # Dynamic interval tiers (milliseconds)
      - SCRAPER_INTERVAL_HIGH_MS=${SCRAPER_INTERVAL_HIGH_MS:-120000}
      - SCRAPER_INTERVAL_MEDIUM_MS=${SCRAPER_INTERVAL_MEDIUM_MS:-300000}
      - SCRAPER_INTERVAL_LOW_MS=${SCRAPER_INTERVAL_LOW_MS:-600000}
      - SCRAPER_INTERVAL_IDLE_MS=${SCRAPER_INTERVAL_IDLE_MS:-900000}
      # Background next-day schedule prefetch interval (default 8 hours)
      - SCRAPER_PREFETCH_INTERVAL_MS=${SCRAPER_PREFETCH_INTERVAL_MS:-28800000}
      # Deprecated — replaced by tiered intervals above
      - SCRAPER_INTERVAL_MS=${SCRAPER_INTERVAL_MS:-300000}
    networks:
      - web
    restart: always
    deploy:
      resources:
        limits:
          memory: 800m

  guernsey-scraper:
    build:
      context: .
      dockerfile: apps/guernsey-scraper/Dockerfile
    environment:
      - DATABASE_URL=${DATABASE_URL}
      - BACKFILL_START_DATE=${BACKFILL_START_DATE:-2019-01-01}
      - BACKFILL_END_DATE=${BACKFILL_END_DATE}
      - GUERNSEY_AIRPORT_URL=${GUERNSEY_AIRPORT_URL:-https://www.airport.gg}
      - NODE_ENV=production
    networks:
      - web
    restart: on-failure
    profiles:
      - backfill

  position-service:
    build:
      context: .
      dockerfile: apps/position-service/Dockerfile
    environment:
      - DATABASE_URL=${DATABASE_URL}
      - FR24_API_TOKEN=${FR24_API_TOKEN}
      - POSITION_INTERVAL_MS=${POSITION_INTERVAL_MS:-300000}
      - NODE_ENV=production
    networks:
      - web
    restart: always

  weather-service:
    build:
      context: .
      dockerfile: apps/weather-service/Dockerfile
    environment:
      - DATABASE_URL=${DATABASE_URL}
      - OPENMETEO_API_URL=${OPENMETEO_API_URL:-https://api.open-meteo.com/v1/forecast}
      - NODE_ENV=production
    networks:
      - web
    restart: always

  ml-service:
    build:
      context: apps/ml-service
      dockerfile: Dockerfile
    expose:
      - "8000"
    environment:
      - DATABASE_URL=${DATABASE_URL}
      - MODEL_PATH=/app/models
    volumes:
      - ml-models-prod:/app/models
    networks:
      - web
    restart: unless-stopped

networks:
  web:
    external: true

volumes:
  ml-models-prod:
